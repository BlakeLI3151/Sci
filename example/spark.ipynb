{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Container Jupyter/all-spark-notebook\n",
    "## Exampe 2.  A K-means computation in Spark\n",
    "This is a simple demo of using spark to compute k-means where k = 4.   To make it easy to repeat the computation with different numbers of point and check the accuracy and performance, we create an artificial set of points in the plane where there are 4 clusters of \"random\" points.   \n",
    "\n",
    "we first import the python spark package and several others we will need.   If pyspark does not load you may have the wrong kernel running.   On the data science vm, look for kernel \"Spark-python\".   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pyspark\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to do some plotting, so let's set that up to.   also let's get the numpy library and call it np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we first create the set of \"n\" points.  Later we will come back and try different values of n, but this is a good place to start.   We create n pairs of the form (0.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "nums = np.zeros((n,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nums.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create four \"random\" clusters.  Each cluster is in the shape of a small circle of points. This is so that we can repeat the experiment and it will converge in the same way each time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(int(n/4)):\n",
    "    x = random()*3.1415*2\n",
    "    s =  0.6\n",
    "    ranx = s*(2*random()-1)\n",
    "    rany = s*(2*random()-1)\n",
    "    nums[4*i] = np.array([1+ np.sin(x)*ranx, np.cos(x)*rany], dtype='f')\n",
    "    x = random()*3.1415*2\n",
    "    ranx = s*(2*random()-1)\n",
    "    rany = s*(2*random()-1)\n",
    "    nums[4*i+1] = np.array([np.sin(x)*ranx,1+ np.cos(x)*rany], dtype='f')\n",
    "    x = random()*3.1415*2\n",
    "    ranx = s*(2*random()-1)\n",
    "    rany = s*(2*random()-1)\n",
    "    nums[4*i+2] = np.array([np.sin(x)*ranx,-1+ np.cos(x)*rany], dtype='f')\n",
    "    x = random()*3.1415*2\n",
    "    ranx = s*(2*random()-1)\n",
    "    rany = s*(2*random()-1)\n",
    "    nums[4*i+3] = np.array([-1+ np.sin(x)*ranx, np.cos(x)*rany], dtype='f')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the spark part starts here.\n",
    "we begin by creating the local context.  On the data science VM using the spark-python kernel the spark local context aready exists and it is called \"sc\".\n",
    "\n",
    "in other situations you need to specifically create one.  If the next command indicates that \"sc\" is not defined, then you need to create it by uncommenting the first line and running this step again.\n",
    "\n",
    "\n",
    "if you want to connect to a cluster the instructions are here https://github.com/jupyter/docker-stacks/tree/master/all-spark-notebook.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sc = pyspark.SparkContext('local[*]')\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create our Spark RDD with the data.   In Spark an RDD is a resilliant distribute dataset.  it is distrubted over a number of partitions.  If you have more than one core  or servers in your cluster, you can use this to share the work.   We will try various numbers of partitions.  Let's start with 4.   Later we can try 8 and 16 to see  how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpartitions = 32\n",
    "data = sc.parallelize(nums, numpartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the data.   To do that we will use the RDD sample method to pull out a samle into a new RDD and apply the collect() function to return a regular array we can plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pts = data.sample(False,0.001,seed=22222).collect()\n",
    "for y in pts:\n",
    "    plt.plot(y[0],y[1],'+')\n",
    "plt.title(\"sample of unclusterd data. sample size = \"+str(len(pts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function takes a point p and an array of candidate cluster centers and returns the index of the nearest center.  this is how we will determine which cluster a point belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def closestPoint(p, centers):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    for i in range(len(centers)):\n",
    "        tempDist = np.sum((p - centers[i]) ** 2)\n",
    "        if tempDist < closest:\n",
    "            closest = tempDist\n",
    "            bestIndex = i\n",
    "    return bestIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will look for k clusters and we pick a set of \"guesses\" for the four starting points.   By starting this way, every time we run this it will be the same so that we can get consistent performance measurements.   The algorithm is very sensitive to the choice of starting points.  Bad choices will result in very bad selections of partitions.  (this is eally not a very good algorithm!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "convergeDist = 0.01\n",
    "kPoints = [[2., 0.], [0.0, 1.0], [-.1,0], [0,-2]]\n",
    "ch = ['r+','g+','b+','c+','y+','k']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is loop that runs until the centers are no longer moving.  This is not the best algorithm for k-means, but it illustrates Spark operations.\n",
    " we start with the data and use the map operation to create a new RDD of the form\n",
    " \n",
    "     { (j, (p, 1)) where j is the index of the closest center to p for each p in data}\n",
    "     \n",
    " we next recuce this so that we have the set\n",
    " \n",
    "    { (j, ( sum(p), sum(1) ) where the sum is over the the number of tuples from closest that have key j}\n",
    "    \n",
    " We can now find the centroid for all the points closest to kpoints[j]  by computing {sum(p)/sum(1)}.   This is an RDD of size K and we can collect it and use it as the kPoints for the next iteration.\n",
    " \n",
    "This algorithm is very sensitive to the initial choice of the centroids kPoints.  A bad choice will produce bizzare clusters.  But the point here is to illustrate Spark using an iterative may reduce algorithm.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tempDist = 1.0\n",
    "numiters = 0\n",
    "start_time = time.time()\n",
    "while tempDist > convergeDist:\n",
    "    closest = data.map(\n",
    "        lambda p: (closestPoint(p, kPoints), (p, 1)))\n",
    "    pointStats = closest.reduceByKey(\n",
    "        lambda x, y : (x[0] + y[0], x[1] + y[1]))\n",
    "    newPoints = pointStats.map(\n",
    "        lambda x : (x[0], x[1][0]/ x[1][1])).collect()\n",
    "    tempDist = sum(np.sum((kPoints[x] - y) ** 2) for (x, y) in newPoints)\n",
    "    for (x, y) in newPoints:\n",
    "        kPoints[x] = y\n",
    "    numiters = numiters+1\n",
    "end_time = time.time()\n",
    "print(\"time = %f\"%(end_time-start_time))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The data below gives the performance for different values of n and the number of partitions.  We used an 8 core server on the NSF JetStream cluster.  and a little dual core Mac mini.  How did your machine do?  You can go back and change the number of partitions without having to recreate the data.    If you increase n you need to rebuild the data.  \n",
    "\n",
    "data for 8 core server on JetStream\n",
    "n = 10000\n",
    "32 partitions time = 1.792782\n",
    "16 partitions time = 1.480762\n",
    "8  partitions time = 0.725798\n",
    "4  partitions time = 0.804337\n",
    "2  partitions time = 0.929703\n",
    "1  partition  time = 1.289854\n",
    "n = 100000\n",
    "1  partition time = 10.309104\n",
    "2  partitions time = 5.397808\n",
    "4  partitions time = 3.580470\n",
    "8  partitions time = 2.199382\n",
    "16 partitions time = 2.574314\n",
    "n = 1000000\n",
    "16 partitions time = 16.601281\n",
    "8  partitions time = 16.107580\n",
    "4  partitions time = 27.401739\n",
    "2  partitions time = 49.731367\n",
    "1  partition  time = 97.472141\n",
    "n = 10000000 crash\n",
    "\n",
    "Data for Mac mini. (dual core 2.6 GHz Intel Core i5)\n",
    "n = 10000\n",
    "32 partitions time = 8.612449\n",
    "16 partitions time = 3.572536\n",
    "8  partitions time = 1.871207\n",
    "4  partitions time = 1.747807\n",
    "2  partitions time = 0.882314\n",
    "1  partition  time = 1.303219\n",
    "\n",
    "n = 100000 \n",
    "16 partitions time = 8.081378\n",
    "8  partitions time = 5.788275\n",
    "4  partitions time = 5.553322\n",
    "2  partitions time = 4.778752\n",
    "1  partition  time = 8.646441\n",
    "\n",
    "\n",
    "n = 1000000\n",
    "1  partition  time = 80.778645\n",
    "2  partitions time = 42.087406\n",
    "4  partitions time = 41.951894\n",
    "8  partitions time = 44.810061\n",
    "16 partitions time = 43.249103\n",
    "32 partitions time = 47.106305\n",
    "\n",
    "n = 10000000 failed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now look at the final clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pts = data.map(\n",
    "    lambda p:(closestPoint(p, kPoints), p)).sample(False,0.001,seed=22222).collect()\n",
    "\n",
    "for (x,y)in pts:\n",
    "    plt.plot(y[0],y[1],ch[x%4])\n",
    "plt.title(\"sample of clstered data\")\n",
    "print(\"Final centers: %s\"%str(kPoints))\n",
    "print(\"number if iterations =%d\"%numiters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for fun, let's look at the performance for various values of n and k on the two machines.  To compare these results we can look at absolute execution times, but it is a bit more interesting to look at speed up as a function of the number of partitions of the SparkRDD.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jet = [\n",
    "[10000,\n",
    "[[1  , 1.289854],\n",
    "[2  , 0.929703],\n",
    "[4  , 0.804337],\n",
    "[8  , 0.725798],\n",
    "[16 , 1.480762]]],\n",
    "[100000,\n",
    "[[1  , 10.309104],\n",
    "[2  , 5.397808],\n",
    "[4  , 3.580470],\n",
    "[8  , 2.199382],\n",
    "[16 , 2.574314]]],\n",
    "[1000000,\n",
    "[[1  , 97.472141],\n",
    "[2  , 49.731367],\n",
    "[4  , 27.401739],\n",
    "[8  , 16.107580],\n",
    "[16 , 16.601281]]]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mac = [[10000,\n",
    "[[1  , 1.303219],\n",
    "[2  , 0.882314],\n",
    "[4  , 1.747807],\n",
    "[8  , 1.871207],\n",
    "[16 , 3.572536]]],\n",
    "[100000,\n",
    "[[1  , 8.646441],\n",
    "[2  , 4.778752],\n",
    "[4  , 5.553322],\n",
    "[8  , 5.788275],\n",
    "[16 , 8.081378]]],\n",
    "[1000000,\n",
    "[[1  , 80.778645],\n",
    "[2  , 42.087406],\n",
    "[4  , 41.951894],\n",
    "[8  , 44.810061],\n",
    "[16 , 43.249103]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parts = [1,2,4,8,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 20, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('fivethirtyeight'):\n",
    "    for i in range(3):\n",
    "        minv = mac[i][1][0][1]\n",
    "        macvals = [minv/x[1] for x in mac[i][1]]\n",
    "        plt.plot(parts, macvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context('fivethirtyeight'):\n",
    "    for i in range(3):\n",
    "        minv = jet[i][1][0][1]\n",
    "        jetvals = [minv/x[1] for x in jet[i][1]]\n",
    "        plt.plot(parts, jetvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the speedup for the 2-core mac is best with 2 partitions and the 8 cores of the JetStream server max out at  8 partitions.   What this says is that while more parallelism is available when the number of partitions is greater than the number of cores, the system cannot take advantage of it because the cores are already saturated with work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - python",
   "language": "python",
   "name": "spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
