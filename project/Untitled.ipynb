{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abs_path = os.path.dirname(os.path.abspath(__file__) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        readme_file = os.path.normpath(os.path.join(\n",
    "            os.path.dirname(path), \"..\", \"README.md\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "       \n",
    "def create_reader(path, is_training, input_dim, label_dim):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n",
    "        features  = StreamDef(field='features', shape=input_dim, is_sparse=False),\n",
    "        labels    = StreamDef(field='labels',   shape=label_dim, is_sparse=False)\n",
    "    )), randomize=is_training, max_sweeps = INFINITELY_REPEAT if is_training else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def simple_mnist(tensorboard_logdir=None):\n",
    "    input_dim = 784\n",
    "    num_output_classes = 10\n",
    "    num_hidden_layers = 1\n",
    "    hidden_layers_dim = 200\n",
    "\n",
    " \n",
    "    feature = C.input_variable(input_dim, np.float32)\n",
    "    label = C.input_variable(num_output_classes, np.float32)\n",
    "\n",
    "\n",
    "    scaled_input = element_times(constant(0.00390625), feature)\n",
    "\n",
    "    z = Sequential([For(range(num_hidden_layers), lambda i: Dense(hidden_layers_dim, activation=relu)),\n",
    "                    Dense(num_output_classes)])(scaled_input)\n",
    "\n",
    "    ce = cross_entropy_with_softmax(z, label)\n",
    "    pe = classification_error(z, label)\n",
    "\n",
    "    data_dir = os.path.join(abs_path, \"..\", \"..\", \"..\", \"DataSets\", \"MNIST\")\n",
    "\n",
    "    path = os.path.normpath(os.path.join(data_dir, \"Train-28x28_cntk_text.txt\"))\n",
    "    check_path(path)\n",
    "\n",
    "    reader_train = create_reader(path, True, input_dim, num_output_classes)\n",
    "\n",
    "    input_map = {\n",
    "        feature  : reader_train.streams.features,\n",
    "        label  : reader_train.streams.labels\n",
    "    }\n",
    "\n",
    "\n",
    "    minibatch_size = 64\n",
    "    num_samples_per_sweep = 60000\n",
    "    num_sweeps_to_train_with = 10\n",
    "\n",
    "\n",
    "    progress_writers = [ProgressPrinter(\n",
    "       \n",
    "        tag='Training',\n",
    "        num_epochs=num_sweeps_to_train_with)]\n",
    "\n",
    "    if tensorboard_logdir is not None:\n",
    "        progress_writers.append(TensorBoardProgressWriter(freq=10, log_dir=tensorboard_logdir, model=z))\n",
    "\n",
    "   \n",
    "    lr = learning_parameter_schedule_per_sample(1)\n",
    "    trainer = Trainer(z, (ce, pe), adadelta(z.parameters, lr), progress_writers)\n",
    "\n",
    "    training_session(\n",
    "        trainer=trainer,\n",
    "        mb_source = reader_train,\n",
    "        mb_size = minibatch_size,\n",
    "        model_inputs_to_streams = input_map,\n",
    "        max_samples = num_samples_per_sweep * num_sweeps_to_train_with,\n",
    "        progress_frequency=num_samples_per_sweep\n",
    "    ).train()\n",
    "\n",
    "    path = os.path.normpath(os.path.join(data_dir, \"Test-28x28_cntk_text.txt\"))\n",
    "    check_path(path)\n",
    "\n",
    "    reader_test = create_reader(path, False, input_dim, num_output_classes)\n",
    "\n",
    "    input_map = {\n",
    "        feature  : reader_test.streams.features,\n",
    "        label  : reader_test.streams.labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    test_minibatch_size = 1024\n",
    "    num_samples = 10000\n",
    "    num_minibatches_to_test = num_samples / test_minibatch_size\n",
    "    test_result = 0.0\n",
    "    for i in range(0, int(num_minibatches_to_test)):\n",
    "        mb = reader_test.next_minibatch(test_minibatch_size, input_map=input_map)\n",
    "        eval_error = trainer.test_minibatch(mb)\n",
    "        test_result = test_result + eval_error\n",
    "\n",
    "    C.debugging.stop_profiler()\n",
    "    trainer.print_node_timing()\n",
    "\n",
    "  \n",
    "    return test_result / num_minibatches_to_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-tensorboard_logdir', '--tensorboard_logdir',\n",
    "                        help='Directory where TensorBoard logs should be created', required=False, default=None)\n",
    "    args = vars(parser.parse_args())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
